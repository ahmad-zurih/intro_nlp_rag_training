{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24f8cdf9-8b01-4220-b4d6-bc98b2a76901",
   "metadata": {},
   "source": [
    "# Exploring a RAG System with ChromaDB and GPUStack (OpenAI-compatible API)\n",
    "\n",
    "This notebook demonstrates a **Retrieval-Augmented Generation (RAG)** pipeline using:\n",
    "\n",
    "- **GPUStack** (OpenAI-compatible HTTP API) for **embeddings** and the **LLM** get api key from this link: https://gpustack.unibe.ch/\n",
    "- **ChromaDB** as the vector database (stores embeddings + metadata)\n",
    "- Helper functions to ingest documents, chunk them, embed them, retrieve relevant chunks, and generate an answer\n",
    "\n",
    "The main goal is to understand *how the pieces fit together*:\n",
    "\n",
    "1. **Ingest** documents → split into chunks  \n",
    "2. **Embed** chunks → store in a vector DB  \n",
    "3. **Retrieve** top-k similar chunks for a user query  \n",
    "4. **Augment** the LLM prompt with retrieved chunks  \n",
    "5. **Generate** a grounded response\n",
    "\n",
    "> **Educational note:** This notebook is designed for teaching. It prioritizes clarity over production best-practices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f608af0a-93e2-4c69-bd79-620836593927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages. Run here or on the terminal\n",
    "\n",
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe43c0ec-c213-4fcd-b4c7-db88b665dc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import nltk\n",
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "# --- GPUStack (OpenAI-compatible) client setup ---\n",
    "api_key = open(\"api-key\", \"r\").read().strip() \n",
    "print(api_key)\n",
    "GPUSTACK_BASE_URL = os.getenv(\"GPUSTACK_BASE_URL\", \"https://gpustack.unibe.ch/v1\")\n",
    "GPUSTACK_API_KEY = os.getenv(\"GPUSTACK_API_KEY\", api_key)\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=GPUSTACK_BASE_URL,\n",
    "    api_key=GPUSTACK_API_KEY,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90af85b4-ca96-4630-9e3d-876b6c55cd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check if your api key if working for embedding\n",
    "\n",
    "test_emb = client.embeddings.create(\n",
    "    model=\"qwen3-embedding-0.6b\",\n",
    "    input=[\"Hello from GPUStack!\"],\n",
    ")\n",
    "\n",
    "print(\"Embedding length:\", len(test_emb.data[0].embedding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc870a33-713a-44fd-a61e-aca431276925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check if your api key if working for LLM\n",
    "\n",
    "test_chat = client.chat.completions.create(\n",
    "    model=\"gpt-oss-120b\",\n",
    "    temperature=0.2,\n",
    "    top_p=1,\n",
    "    max_tokens=200,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"In one sentence, explain what RAG is.\"}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(test_chat.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b735808b-d3ec-40b3-9bc0-56481a7016a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set some variables\n",
    "\n",
    "# GPUStack models (OpenAI-compatible)\n",
    "embedding_model_name = \"qwen3-embedding-0.6b\"\n",
    "llm_model = \"gpt-oss-120b\"\n",
    "\n",
    "vector_db = \"chromaDB\"  \n",
    "\n",
    "collection_name = \"dsl_embeddings_gpustack_demo_1\"\n",
    "\n",
    "raw_db = \"/home/ahmad-unibe/dsl_data\"  # root directory where raw documents are stored\n",
    "\n",
    "data_language = \"english\"  # tokenizer language for sentence splitting\n",
    "\n",
    "db_directory = os.path.join(os.path.expanduser('~'), '.db')  # where ChromaDB will be stored locally\n",
    "chunk_size = 20  # number of sentences per chunk\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are a helpful, polite assistant that works at the Data Science Lab (DSL).\n",
    "\n",
    "You will receive:\n",
    "1) Retrieved context chunks (may be partial and messy).\n",
    "2) A user question.\n",
    "\n",
    "Use the context to answer as well as you can.\n",
    "- If the context does not contain enough information, say you don't know.\n",
    "- If appropriate, advise contacting DSL via info.dsl@unibe.ch or support.dsl@unibe.ch.\n",
    "\n",
    "---\n",
    "CONTEXT:\n",
    "{data}\n",
    "\n",
    "---\n",
    "QUESTION:\n",
    "{query}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9616d933-5ac0-4276-a072-954fab171cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions: read raw files (txt/pdf), split to sentences, chunk sentences\n",
    "\n",
    "def get_file_paths(root_dir: str, file_extensions: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Retrieves a list of paths to all files with specified extensions in the given root directory and its subdirectories.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): The root directory to search for files.\n",
    "        file_extensions (list[str]): A list of file extensions to retrieve. For example, [\"txt\", \"pdf\"]\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of file paths to all matching files found within the root directory and its subdirectories.\n",
    "    \"\"\"\n",
    "    file_paths = []\n",
    "    \n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if any(filename.endswith(f\".{ext}\") for ext in file_extensions):\n",
    "                file_paths.append(os.path.join(dirpath, filename))\n",
    "    \n",
    "    return file_paths\n",
    "\n",
    "\n",
    "\n",
    "def read_text_file(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads the content of a text file and returns it as a single string.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the .txt file to read.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the file as a single string.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    return content\n",
    "\n",
    "\n",
    "def read_pdf_file(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads the content of a PDF file and returns it as a single string.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the PDF file to read.\n",
    "    \n",
    "    Returns:\n",
    "        str: The content of the PDF as a single string.\n",
    "    \"\"\"\n",
    "    text_content = []\n",
    "    \n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            # Extract text from each page\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:  # Ensure the page has text\n",
    "                text_content.append(page_text)\n",
    "    \n",
    "    # Join all pages' text into a single string\n",
    "    return \"\\n\".join(text_content)\n",
    "\n",
    "\n",
    "def split_text_into_sentences(text: str, language: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Splits the given text into a list of sentences using NLTK's sentence tokenizer.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to split into sentences.\n",
    "        language (str): The language of the text for the sentence tokenizer\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of sentences.\n",
    "    \"\"\"\n",
    "    sentences = nltk.sent_tokenize(text, language=language)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def chunk_sentences(sentences: list[str], chunk_size: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Groups a list of sentences into chunks, each containing up to `chunk_size` sentences.\n",
    "\n",
    "    Args:\n",
    "        sentences (list[str]): A list of sentences.\n",
    "        chunk_size (int): The number of sentences per chunk.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of text chunks, each containing up to `chunk_size` sentences.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(sentences), chunk_size):\n",
    "        chunk = \" \".join(sentences[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f57fbc3-7c26-42f0-8c1b-f875c856adff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Embedding and storing documents in ChromaDB (GPUStack embeddings) ---\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=db_directory)\n",
    "\n",
    "print(\"\\n--- Embedding and Storing Documents in ChromaDB ---\")\n",
    "print(f\"Embedding Model (GPUStack): {embedding_model_name}\")\n",
    "print(f\"Chunk Size (sentences per chunk): {chunk_size}\")\n",
    "print(f\"Raw Data Directory: {raw_db}\")\n",
    "print(f\"Vector Database Directory: {db_directory}\\n\")\n",
    "print(f\"Vector Database is: {vector_db}\\n\")\n",
    "\n",
    "# Step 1: Load documents (txt and pdf)\n",
    "file_paths = get_file_paths(raw_db, [\"txt\", \"pdf\"])\n",
    "print(f\"Found {len(file_paths)} files to process.\\n\")\n",
    "\n",
    "# Create or retrieve the collection in ChromaDB\n",
    "collection = chroma_client.get_or_create_collection(collection_name)\n",
    "\n",
    "def embed_texts(texts: list[str], model: str, batch_size: int = 32) -> list[list[float]]:\n",
    "    \"\"\"Embed a list of texts using GPUStack embeddings API, in batches.\"\"\"\n",
    "    all_embeddings: list[list[float]] = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        resp = client.embeddings.create(model=model, input=batch)\n",
    "        all_embeddings.extend([d.embedding for d in resp.data])\n",
    "    return all_embeddings\n",
    "\n",
    "for file_path in tqdm(file_paths, desc=\"Processing documents\"):\n",
    "    # Step 2: Read content based on file type\n",
    "    if file_path.endswith('.txt'):\n",
    "        text = read_text_file(file_path)\n",
    "    elif file_path.endswith('.pdf'):\n",
    "        text = read_pdf_file(file_path)\n",
    "    else:\n",
    "        print(f\"Unsupported file type: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    # Step 3: Split text into sentences\n",
    "    sentences = split_text_into_sentences(text, data_language)\n",
    "\n",
    "    # Step 4: Chunk sentences into groups\n",
    "    chunks = chunk_sentences(sentences, chunk_size)\n",
    "\n",
    "    # Step 5: Embed each chunk (batch for efficiency)\n",
    "    embeddings = embed_texts(chunks, model=embedding_model_name, batch_size=32)\n",
    "\n",
    "    # Use file name as the document ID and create metadata with chunk index\n",
    "    file_name = os.path.basename(file_path)\n",
    "    ids = []\n",
    "    metadatas = []\n",
    "    for i, _ in enumerate(chunks):\n",
    "        chunk_id = f\"{file_name}_chunk_{i}\"\n",
    "        ids.append(chunk_id)\n",
    "        metadatas.append({\"file_name\": file_name, \"chunk_id\": chunk_id, \"chunk_index\": i})\n",
    "\n",
    "    # Step 6: Add to ChromaDB\n",
    "    collection.add(\n",
    "        ids=ids,\n",
    "        documents=chunks,\n",
    "        embeddings=embeddings,\n",
    "        metadatas=metadatas,\n",
    "    )\n",
    "\n",
    "print(\"Done! Documents embedded and stored in ChromaDB.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c8cdf-748f-4f76-b576-537b73e30fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChromaRetriever:\n",
    "    \"\"\"Retrieve documents from a ChromaDB collection using GPUStack embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model: str, db_path: str, db_collection: str, n_results: int) -> None:\n",
    "        self.embedding_model = embedding_model\n",
    "        self.db_path = db_path\n",
    "        self.db_collection = db_collection\n",
    "        self.n_results = n_results\n",
    "\n",
    "        self.client = chromadb.PersistentClient(path=self.db_path)\n",
    "        self.collection = self.client.get_collection(name=self.db_collection)\n",
    "\n",
    "    def _embed(self, text: str) -> list[float]:\n",
    "        resp = client.embeddings.create(model=self.embedding_model, input=[text])\n",
    "        return resp.data[0].embedding\n",
    "\n",
    "    def retrieve(self, query: str):\n",
    "        \"\"\"Embeds the query and retrieves relevant documents from the collection.\"\"\"\n",
    "        try:\n",
    "            embedded_query = self._embed(query)\n",
    "            results = self.collection.query(\n",
    "                query_embeddings=[embedded_query],\n",
    "                n_results=self.n_results\n",
    "            )\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during retrieval: {e}\")\n",
    "            return None\n",
    "\n",
    "    def format_results_for_prompt(self, results) -> str:\n",
    "        \"\"\"Format retrieved chunks into a readable context string for the LLM prompt.\"\"\"\n",
    "        if not results or not results.get(\"documents\"):\n",
    "            return \"No relevant data found.\"\n",
    "\n",
    "        formatted_data = \"\"\n",
    "        for idx, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n",
    "            chunk_id = metadata.get('chunk_id', 'N/A')\n",
    "            file_name = metadata.get('file_name', 'N/A')\n",
    "            formatted_data += f\"Document {idx + 1}:\\n\"\n",
    "            formatted_data += f\"Chunk ID: {chunk_id}\\n\"\n",
    "            formatted_data += f\"File Name: {file_name}\\n\"\n",
    "            formatted_data += f\"Content:\\n{doc}\\n\"\n",
    "            formatted_data += \"-\" * 60 + \"\\n\"\n",
    "        return formatted_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65addc8c-b792-4bff-ae1b-70584a543b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Responder:\n",
    "    \"\"\"Generate responses using a GPUStack-hosted LLM (OpenAI-compatible chat API).\"\"\"\n",
    "\n",
    "    def __init__(self, data: str, model: str, prompt_template: str, query: str) -> None:\n",
    "        self.data = data\n",
    "        self.model = model\n",
    "        self.prompt_template = prompt_template\n",
    "        self.query = query\n",
    "        self.prompt = prompt_template.format(data=self.data, query=self.query)\n",
    "\n",
    "    def generate_response(self) -> str:\n",
    "        \"\"\"One-shot response generation.\"\"\"\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                temperature=0.3,\n",
    "                top_p=1,\n",
    "                max_tokens=800,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": self.prompt}\n",
    "                ],\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"An error occurred during response generation: {e}\")\n",
    "\n",
    "    def stream_response(self):\n",
    "        \"\"\"Stream the response token-by-token (if supported by the server).\"\"\"\n",
    "        try:\n",
    "            stream = client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                temperature=0.3,\n",
    "                top_p=1,\n",
    "                max_tokens=800,\n",
    "                messages=[{\"role\": \"user\", \"content\": self.prompt}],\n",
    "                stream=True,\n",
    "            )\n",
    "            for event in stream:\n",
    "                delta = event.choices[0].delta\n",
    "                if delta and getattr(delta, \"content\", None):\n",
    "                    print(delta.content, end=\"\", flush=True)\n",
    "            print(\"\\n\")\n",
    "        except TypeError:\n",
    "            # Some OpenAI-compatible servers do not support streaming.\n",
    "            print(self.generate_response())\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"An error occurred during streaming: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7f8039-c85c-4707-9c0b-9f294182d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's try the retriever by itself first\n",
    "\n",
    "results_numbers = 5\n",
    "\n",
    "retriever = ChromaRetriever(embedding_model=embedding_model_name, \n",
    "                                db_path=db_directory, \n",
    "                                db_collection=collection_name, \n",
    "                                n_results=results_numbers)\n",
    "\n",
    "while True:\n",
    "    query = str(input(\"Type a query to search the DB. Type 'quit' to exit:  \"))\n",
    "\n",
    "    if query.lower() == 'quit':\n",
    "        break\n",
    "    else:\n",
    "        results = retriever.retrieve(query)\n",
    "\n",
    "\n",
    "            # Print out the results\n",
    "        print(\"\\n--- Query Results ---\\n\")\n",
    "        for idx, (doc, metadata, distance) in enumerate(zip(results['documents'][0], results['metadatas'][0], results['distances'][0])):\n",
    "            print(f\"Result {idx + 1}:\")\n",
    "            print(f\"Document ID: {metadata.get('chunk_id', 'N/A')}\")\n",
    "            print(f\"File Name: {metadata.get('file_name', 'N/A')}\")\n",
    "            print(f\"Distance: {distance}\")\n",
    "            print(f\"Content:\\n{doc}\\n\")\n",
    "            print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc5ae6-2871-40db-94d1-43ee77119889",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try the whole\n",
    "while True:\n",
    "    retriever = ChromaRetriever(embedding_model=embedding_model_name, \n",
    "                                db_path=db_directory, \n",
    "                                db_collection=collection_name, \n",
    "                                n_results=5)\n",
    "        \n",
    "    user_query = str(input(\"Ask a question. Type quit to exit:  \"))\n",
    "    if user_query.lower() == \"quit\":\n",
    "        break\n",
    "    else:\n",
    "        print(\"Looking the DB for relevant information .......\")\n",
    "        # get the data for the RAG and put it in str format\n",
    "        search_results = retriever.retrieve(user_query)\n",
    "        formated_result = retriever.format_results_for_prompt(search_results)\n",
    "\n",
    "        responder = Responder(data=formated_result, model=llm_model, prompt_template=prompt, query=user_query)\n",
    "        responder.stream_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4479e074-c848-48a3-8821-ea0a335b102d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
