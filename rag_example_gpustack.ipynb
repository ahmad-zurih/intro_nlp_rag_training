{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24f8cdf9-8b01-4220-b4d6-bc98b2a76901",
   "metadata": {},
   "source": [
    "# Exploring a RAG System with ChromaDB and GPUStack (OpenAI-compatible API)\n",
    "\n",
    "This notebook demonstrates a **Retrieval-Augmented Generation (RAG)** pipeline using:\n",
    "\n",
    "- **GPUStack** (OpenAI-compatible HTTP API) for **embeddings** and the **LLM** get api key from this link: https://gpustack.unibe.ch/\n",
    "- **ChromaDB** as the vector database (stores embeddings + metadata)\n",
    "- Helper functions to ingest documents, chunk them, embed them, retrieve relevant chunks, and generate an answer\n",
    "\n",
    "The main goal is to understand *how the pieces fit together*:\n",
    "\n",
    "1. **Ingest** documents → split into chunks  \n",
    "2. **Embed** chunks → store in a vector DB  \n",
    "3. **Retrieve** top-k similar chunks for a user query  \n",
    "4. **Augment** the LLM prompt with retrieved chunks  \n",
    "5. **Generate** a grounded response\n",
    "\n",
    "> **Educational note:** This notebook is designed for teaching. It prioritizes clarity over production best-practices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f608af0a-93e2-4c69-bd79-620836593927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (2.17.0)\n",
      "Requirement already satisfied: notebook in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (7.5.3)\n",
      "Requirement already satisfied: chromadb in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (1.5.0)\n",
      "Requirement already satisfied: pdfplumber in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (0.11.4)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (4.67.3)\n",
      "Requirement already satisfied: nltk in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (3.9.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.12/site-packages (from openai->-r requirements.txt (line 1)) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.12/site-packages (from openai->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.12/site-packages (from openai->-r requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in ./venv/lib/python3.12/site-packages (from openai->-r requirements.txt (line 1)) (0.13.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./venv/lib/python3.12/site-packages (from openai->-r requirements.txt (line 1)) (2.12.5)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.12/site-packages (from openai->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./venv/lib/python3.12/site-packages (from openai->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in ./venv/lib/python3.12/site-packages (from notebook->-r requirements.txt (line 2)) (2.17.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.28.0 in ./venv/lib/python3.12/site-packages (from notebook->-r requirements.txt (line 2)) (2.28.0)\n",
      "Requirement already satisfied: jupyterlab<4.6,>=4.5.3 in ./venv/lib/python3.12/site-packages (from notebook->-r requirements.txt (line 2)) (4.5.3)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in ./venv/lib/python3.12/site-packages (from notebook->-r requirements.txt (line 2)) (0.2.4)\n",
      "Requirement already satisfied: tornado>=6.2.0 in ./venv/lib/python3.12/site-packages (from notebook->-r requirements.txt (line 2)) (6.5.4)\n",
      "Requirement already satisfied: build>=1.0.3 in ./venv/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in ./venv/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 3)) (1.4.3)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in ./venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 3)) (0.40.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in ./venv/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 3)) (2.4.2)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in ./venv/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 3)) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in ./venv/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 3)) (1.24.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in ./venv/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 3)) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in ./venv/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 3)) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in ./venv/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 3)) (1.39.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./venv/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 3)) (0.22.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in ./venv/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 3)) (0.51.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./venv/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 3)) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in ./venv/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 3)) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in ./venv/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 3)) (1.78.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in ./venv/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 3)) (5.0.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in ./venv/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 3)) (0.21.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in ./venv/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 3)) (35.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in ./venv/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 3)) (9.1.4)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in ./venv/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 3)) (6.0.3)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in ./venv/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 3)) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in ./venv/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 3)) (3.11.7)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./venv/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 3)) (14.3.2)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in ./venv/lib/python3.12/site-packages (from chromadb->-r requirements.txt (line 3)) (4.26.0)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in ./venv/lib/python3.12/site-packages (from pdfplumber->-r requirements.txt (line 4)) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in ./venv/lib/python3.12/site-packages (from pdfplumber->-r requirements.txt (line 4)) (12.1.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in ./venv/lib/python3.12/site-packages (from pdfplumber->-r requirements.txt (line 4)) (5.4.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in ./venv/lib/python3.12/site-packages (from pdfminer.six==20231228->pdfplumber->-r requirements.txt (line 4)) (3.4.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in ./venv/lib/python3.12/site-packages (from pdfminer.six==20231228->pdfplumber->-r requirements.txt (line 4)) (46.0.4)\n",
      "Requirement already satisfied: click in ./venv/lib/python3.12/site-packages (from nltk->-r requirements.txt (line 6)) (8.3.1)\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.12/site-packages (from nltk->-r requirements.txt (line 6)) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./venv/lib/python3.12/site-packages (from nltk->-r requirements.txt (line 6)) (2026.1.15)\n",
      "Requirement already satisfied: idna>=2.8 in ./venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 1)) (3.11)\n",
      "Requirement already satisfied: packaging>=24.0 in ./venv/lib/python3.12/site-packages (from build>=1.0.3->chromadb->-r requirements.txt (line 3)) (26.0)\n",
      "Requirement already satisfied: pyproject_hooks in ./venv/lib/python3.12/site-packages (from build>=1.0.3->chromadb->-r requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb->-r requirements.txt (line 3)) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb->-r requirements.txt (line 3)) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb->-r requirements.txt (line 3)) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in ./venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb->-r requirements.txt (line 3)) (0.30.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in ./venv/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (25.1.0)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in ./venv/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (3.1.6)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in ./venv/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (8.8.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./venv/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (5.9.1)\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in ./venv/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (0.12.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in ./venv/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (0.5.4)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in ./venv/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (7.17.0)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in ./venv/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (5.10.4)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in ./venv/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (0.24.1)\n",
      "Requirement already satisfied: pyzmq>=24 in ./venv/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (27.1.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in ./venv/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (2.1.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in ./venv/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (0.18.1)\n",
      "Requirement already satisfied: traitlets>=5.6.0 in ./venv/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (5.14.3)\n",
      "Requirement already satisfied: websocket-client>=1.7 in ./venv/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (1.9.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in ./venv/lib/python3.12/site-packages (from jupyterlab<4.6,>=4.5.3->notebook->-r requirements.txt (line 2)) (2.1.0)\n",
      "Requirement already satisfied: ipykernel!=6.30.0,>=6.5.0 in ./venv/lib/python3.12/site-packages (from jupyterlab<4.6,>=4.5.3->notebook->-r requirements.txt (line 2)) (7.2.0)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in ./venv/lib/python3.12/site-packages (from jupyterlab<4.6,>=4.5.3->notebook->-r requirements.txt (line 2)) (2.3.0)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in ./venv/lib/python3.12/site-packages (from jupyterlab<4.6,>=4.5.3->notebook->-r requirements.txt (line 2)) (82.0.0)\n",
      "Requirement already satisfied: babel>=2.10 in ./venv/lib/python3.12/site-packages (from jupyterlab-server<3,>=2.28.0->notebook->-r requirements.txt (line 2)) (2.18.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in ./venv/lib/python3.12/site-packages (from jupyterlab-server<3,>=2.28.0->notebook->-r requirements.txt (line 2)) (0.13.0)\n",
      "Requirement already satisfied: requests>=2.31 in ./venv/lib/python3.12/site-packages (from jupyterlab-server<3,>=2.28.0->notebook->-r requirements.txt (line 2)) (2.32.5)\n",
      "Requirement already satisfied: six>=1.9.0 in ./venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 3)) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in ./venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: requests-oauthlib in ./venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 3)) (2.0.0)\n",
      "Requirement already satisfied: urllib3!=2.6.0,>=1.24.2 in ./venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 3)) (2.6.3)\n",
      "Requirement already satisfied: durationpy>=0.7 in ./venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 3)) (0.10)\n",
      "Requirement already satisfied: flatbuffers in ./venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 3)) (25.12.19)\n",
      "Requirement already satisfied: protobuf in ./venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 3)) (6.33.5)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in ./venv/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 3)) (8.7.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in ./venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 3)) (1.72.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.39.1 in ./venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 3)) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.39.1 in ./venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 3)) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b1 in ./venv/lib/python3.12/site-packages (from opentelemetry-sdk>=1.2.0->chromadb->-r requirements.txt (line 3)) (0.60b1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./venv/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb->-r requirements.txt (line 3)) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in ./venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 1)) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 1)) (0.4.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.12/site-packages (from rich>=10.11.0->chromadb->-r requirements.txt (line 3)) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.12/site-packages (from rich>=10.11.0->chromadb->-r requirements.txt (line 3)) (2.19.2)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in ./venv/lib/python3.12/site-packages (from tokenizers>=0.13.2->chromadb->-r requirements.txt (line 3)) (1.4.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./venv/lib/python3.12/site-packages (from typer>=0.9.0->chromadb->-r requirements.txt (line 3)) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in ./venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 3)) (0.7.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in ./venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 3)) (1.2.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in ./venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 3)) (0.22.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 3)) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in ./venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 3)) (16.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in ./venv/lib/python3.12/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (25.1.0)\n",
      "Requirement already satisfied: cffi>=2.0.0 in ./venv/lib/python3.12/site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber->-r requirements.txt (line 4)) (2.0.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb->-r requirements.txt (line 3)) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb->-r requirements.txt (line 3)) (2026.2.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in ./venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb->-r requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: typer-slim in ./venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb->-r requirements.txt (line 3)) (0.21.1)\n",
      "Requirement already satisfied: zipp>=3.20 in ./venv/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 3)) (3.23.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in ./venv/lib/python3.12/site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.6,>=4.5.3->notebook->-r requirements.txt (line 2)) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in ./venv/lib/python3.12/site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.6,>=4.5.3->notebook->-r requirements.txt (line 2)) (1.8.20)\n",
      "Requirement already satisfied: ipython>=7.23.1 in ./venv/lib/python3.12/site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.6,>=4.5.3->notebook->-r requirements.txt (line 2)) (9.10.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in ./venv/lib/python3.12/site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.6,>=4.5.3->notebook->-r requirements.txt (line 2)) (0.2.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in ./venv/lib/python3.12/site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.6,>=4.5.3->notebook->-r requirements.txt (line 2)) (1.6.0)\n",
      "Requirement already satisfied: psutil>=5.7 in ./venv/lib/python3.12/site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.6,>=4.5.3->notebook->-r requirements.txt (line 2)) (7.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2>=3.0.3->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (3.0.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./venv/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (4.5.1)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in ./venv/lib/python3.12/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (4.0.0)\n",
      "Requirement already satisfied: rfc3339-validator in ./venv/lib/python3.12/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in ./venv/lib/python3.12/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (0.1.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb->-r requirements.txt (line 3)) (0.1.2)\n",
      "Requirement already satisfied: beautifulsoup4 in ./venv/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (4.14.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in ./venv/lib/python3.12/site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (6.3.0)\n",
      "Requirement already satisfied: defusedxml in ./venv/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in ./venv/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in ./venv/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (3.2.0)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in ./venv/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (0.10.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in ./venv/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (1.5.1)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in ./venv/lib/python3.12/site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (2.21.2)\n",
      "Requirement already satisfied: ptyprocess in ./venv/lib/python3.12/site-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./venv/lib/python3.12/site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 3)) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: webencodings in ./venv/lib/python3.12/site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in ./venv/lib/python3.12/site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: pycparser in ./venv/lib/python3.12/site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber->-r requirements.txt (line 4)) (3.0)\n",
      "Requirement already satisfied: decorator>=4.3.2 in ./venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.6,>=4.5.3->notebook->-r requirements.txt (line 2)) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in ./venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.6,>=4.5.3->notebook->-r requirements.txt (line 2)) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in ./venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.6,>=4.5.3->notebook->-r requirements.txt (line 2)) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in ./venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.6,>=4.5.3->notebook->-r requirements.txt (line 2)) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.6,>=4.5.3->notebook->-r requirements.txt (line 2)) (3.0.52)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in ./venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.6,>=4.5.3->notebook->-r requirements.txt (line 2)) (0.6.3)\n",
      "Requirement already satisfied: fqdn in ./venv/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (1.5.1)\n",
      "Requirement already satisfied: isoduration in ./venv/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in ./venv/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: rfc3987-syntax>=1.1.0 in ./venv/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (1.1.0)\n",
      "Requirement already satisfied: uri-template in ./venv/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in ./venv/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (25.10.0)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in ./venv/lib/python3.12/site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (2.8.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./venv/lib/python3.12/site-packages (from jedi>=0.18.1->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.6,>=4.5.3->notebook->-r requirements.txt (line 2)) (0.8.5)\n",
      "Requirement already satisfied: wcwidth in ./venv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.6,>=4.5.3->notebook->-r requirements.txt (line 2)) (0.6.0)\n",
      "Requirement already satisfied: lark>=1.2.2 in ./venv/lib/python3.12/site-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./venv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.6,>=4.5.3->notebook->-r requirements.txt (line 2)) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./venv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.6,>=4.5.3->notebook->-r requirements.txt (line 2)) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in ./venv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.6,>=4.5.3->notebook->-r requirements.txt (line 2)) (0.2.3)\n",
      "Requirement already satisfied: arrow>=0.15.0 in ./venv/lib/python3.12/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: tzdata in ./venv/lib/python3.12/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (2025.3)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages. Run here or on the terminal\n",
    "\n",
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe43c0ec-c213-4fcd-b4c7-db88b665dc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpustack_8d3a98ebeddc37f2_cbfef2dd7573028829b3b3f24037d197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import nltk\n",
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "# --- GPUStack (OpenAI-compatible) client setup ---\n",
    "api_key = open(\"api-key\", \"r\").read().strip() \n",
    "print(api_key)\n",
    "GPUSTACK_BASE_URL = os.getenv(\"GPUSTACK_BASE_URL\", \"https://gpustack.unibe.ch/v1\")\n",
    "GPUSTACK_API_KEY = os.getenv(\"GPUSTACK_API_KEY\", api_key)\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=GPUSTACK_BASE_URL,\n",
    "    api_key=GPUSTACK_API_KEY,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90af85b4-ca96-4630-9e3d-876b6c55cd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length: 1024\n"
     ]
    }
   ],
   "source": [
    "# Quick check if your api key if working for embedding\n",
    "\n",
    "test_emb = client.embeddings.create(\n",
    "    model=\"qwen3-embedding-0.6b\",\n",
    "    input=[\"Hello from GPUStack!\"],\n",
    ")\n",
    "\n",
    "print(\"Embedding length:\", len(test_emb.data[0].embedding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc870a33-713a-44fd-a61e-aca431276925",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLocalProtocolError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intro_nlp_rag_training/venv/lib/python3.12/site-packages/httpx/_transports/default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intro_nlp_rag_training/venv/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intro_nlp_rag_training/venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intro_nlp_rag_training/venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intro_nlp_rag_training/venv/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intro_nlp_rag_training/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intro_nlp_rag_training/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:86\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     84\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msend_request_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     85\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_request_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33msend_request_body\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intro_nlp_rag_training/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:144\u001b[39m, in \u001b[36mHTTP11Connection._send_request_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    142\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mwrite\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43mh11\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLocalProtocolError\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mLocalProtocolError\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mh11\u001b[49m\u001b[43m.\u001b[49m\u001b[43mRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intro_nlp_rag_training/venv/lib/python3.12/site-packages/httpcore/_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mLocalProtocolError\u001b[39m: Illegal header value b'Bearer gpustack_8d3a98ebeddc37f2_cbfef2dd7573028829b3b3f24037d197\\n'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mLocalProtocolError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intro_nlp_rag_training/venv/lib/python3.12/site-packages/openai/_base_client.py:1005\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1004\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1005\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intro_nlp_rag_training/venv/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intro_nlp_rag_training/venv/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intro_nlp_rag_training/venv/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intro_nlp_rag_training/venv/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intro_nlp_rag_training/venv/lib/python3.12/site-packages/httpx/_transports/default.py:249\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_httpcore_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intro_nlp_rag_training/venv/lib/python3.12/site-packages/httpx/_transports/default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mLocalProtocolError\u001b[39m: Illegal header value b'Bearer gpustack_8d3a98ebeddc37f2_cbfef2dd7573028829b3b3f24037d197\\n'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mAPIConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Quick check if your api key if working for LLM\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m test_chat = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-oss-120b\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mIn one sentence, explain what RAG is.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(test_chat.choices[\u001b[32m0\u001b[39m].message.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intro_nlp_rag_training/venv/lib/python3.12/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intro_nlp_rag_training/venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1192\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1189\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1190\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1191\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intro_nlp_rag_training/venv/lib/python3.12/site-packages/openai/_base_client.py:1297\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, content, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1288\u001b[39m     warnings.warn(\n\u001b[32m   1289\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing raw bytes as `body` is deprecated and will be removed in a future version. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1290\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease pass raw bytes via the `content` parameter instead.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1291\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m   1292\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m   1293\u001b[39m     )\n\u001b[32m   1294\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1295\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, content=content, files=to_httpx_files(files), **options\n\u001b[32m   1296\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/intro_nlp_rag_training/venv/lib/python3.12/site-packages/openai/_base_client.py:1037\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1034\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1036\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRaising connection error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request=request) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   1039\u001b[39m log.debug(\n\u001b[32m   1040\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mHTTP Response: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m   1041\u001b[39m     request.method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1045\u001b[39m     response.headers,\n\u001b[32m   1046\u001b[39m )\n\u001b[32m   1047\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mrequest_id: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, response.headers.get(\u001b[33m\"\u001b[39m\u001b[33mx-request-id\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mAPIConnectionError\u001b[39m: Connection error."
     ]
    }
   ],
   "source": [
    "# Quick check if your api key if working for LLM\n",
    "\n",
    "test_chat = client.chat.completions.create(\n",
    "    model=\"gpt-oss-120b\",\n",
    "    temperature=0.2,\n",
    "    top_p=1,\n",
    "    max_tokens=200,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"In one sentence, explain what RAG is.\"}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(test_chat.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b735808b-d3ec-40b3-9bc0-56481a7016a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set some variables\n",
    "\n",
    "# GPUStack models (OpenAI-compatible)\n",
    "embedding_model_name = \"qwen3-embedding-0.6b\"\n",
    "llm_model = \"gpt-oss-120b\"\n",
    "\n",
    "vector_db = \"chromaDB\"  \n",
    "\n",
    "collection_name = \"dsl_embeddings_gpustack_demo_1\"\n",
    "\n",
    "raw_db = \"/home/ahmad-unibe/dsl_data\"  # root directory where raw documents are stored\n",
    "\n",
    "data_language = \"english\"  # tokenizer language for sentence splitting\n",
    "\n",
    "db_directory = os.path.join(os.path.expanduser('~'), '.db')  # where ChromaDB will be stored locally\n",
    "chunk_size = 20  # number of sentences per chunk\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are a helpful, polite assistant that works at the Data Science Lab (DSL).\n",
    "\n",
    "You will receive:\n",
    "1) Retrieved context chunks (may be partial and messy).\n",
    "2) A user question.\n",
    "\n",
    "Use the context to answer as well as you can.\n",
    "- If the context does not contain enough information, say you don't know.\n",
    "- If appropriate, advise contacting DSL via info.dsl@unibe.ch or support.dsl@unibe.ch.\n",
    "\n",
    "---\n",
    "CONTEXT:\n",
    "{data}\n",
    "\n",
    "---\n",
    "QUESTION:\n",
    "{query}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9616d933-5ac0-4276-a072-954fab171cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions: read raw files (txt/pdf), split to sentences, chunk sentences\n",
    "\n",
    "def get_file_paths(root_dir: str, file_extensions: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Retrieves a list of paths to all files with specified extensions in the given root directory and its subdirectories.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): The root directory to search for files.\n",
    "        file_extensions (list[str]): A list of file extensions to retrieve. For example, [\"txt\", \"pdf\"]\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of file paths to all matching files found within the root directory and its subdirectories.\n",
    "    \"\"\"\n",
    "    file_paths = []\n",
    "    \n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if any(filename.endswith(f\".{ext}\") for ext in file_extensions):\n",
    "                file_paths.append(os.path.join(dirpath, filename))\n",
    "    \n",
    "    return file_paths\n",
    "\n",
    "\n",
    "\n",
    "def read_text_file(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads the content of a text file and returns it as a single string.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the .txt file to read.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the file as a single string.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    return content\n",
    "\n",
    "\n",
    "def read_pdf_file(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads the content of a PDF file and returns it as a single string.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the PDF file to read.\n",
    "    \n",
    "    Returns:\n",
    "        str: The content of the PDF as a single string.\n",
    "    \"\"\"\n",
    "    text_content = []\n",
    "    \n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            # Extract text from each page\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:  # Ensure the page has text\n",
    "                text_content.append(page_text)\n",
    "    \n",
    "    # Join all pages' text into a single string\n",
    "    return \"\\n\".join(text_content)\n",
    "\n",
    "\n",
    "def split_text_into_sentences(text: str, language: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Splits the given text into a list of sentences using NLTK's sentence tokenizer.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to split into sentences.\n",
    "        language (str): The language of the text for the sentence tokenizer\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of sentences.\n",
    "    \"\"\"\n",
    "    sentences = nltk.sent_tokenize(text, language=language)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def chunk_sentences(sentences: list[str], chunk_size: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Groups a list of sentences into chunks, each containing up to `chunk_size` sentences.\n",
    "\n",
    "    Args:\n",
    "        sentences (list[str]): A list of sentences.\n",
    "        chunk_size (int): The number of sentences per chunk.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of text chunks, each containing up to `chunk_size` sentences.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(sentences), chunk_size):\n",
    "        chunk = \" \".join(sentences[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f57fbc3-7c26-42f0-8c1b-f875c856adff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Embedding and Storing Documents in ChromaDB ---\n",
      "Embedding Model (GPUStack): qwen3-embedding-0.6b\n",
      "Chunk Size (sentences per chunk): 20\n",
      "Raw Data Directory: /home/ahmad-unibe/dsl_data\n",
      "Vector Database Directory: /home/ahmad-unibe/.db\n",
      "\n",
      "Vector Database is: chromaDB\n",
      "\n",
      "Found 18 files to process.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 18/18 [00:09<00:00,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Documents embedded and stored in ChromaDB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Embedding and storing documents in ChromaDB (GPUStack embeddings) ---\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=db_directory)\n",
    "\n",
    "print(\"\\n--- Embedding and Storing Documents in ChromaDB ---\")\n",
    "print(f\"Embedding Model (GPUStack): {embedding_model_name}\")\n",
    "print(f\"Chunk Size (sentences per chunk): {chunk_size}\")\n",
    "print(f\"Raw Data Directory: {raw_db}\")\n",
    "print(f\"Vector Database Directory: {db_directory}\\n\")\n",
    "print(f\"Vector Database is: {vector_db}\\n\")\n",
    "\n",
    "# Step 1: Load documents (txt and pdf)\n",
    "file_paths = get_file_paths(raw_db, [\"txt\", \"pdf\"])\n",
    "print(f\"Found {len(file_paths)} files to process.\\n\")\n",
    "\n",
    "# Create or retrieve the collection in ChromaDB\n",
    "collection = chroma_client.get_or_create_collection(collection_name)\n",
    "\n",
    "def embed_texts(texts: list[str], model: str, batch_size: int = 32) -> list[list[float]]:\n",
    "    \"\"\"Embed a list of texts using GPUStack embeddings API, in batches.\"\"\"\n",
    "    all_embeddings: list[list[float]] = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        resp = client.embeddings.create(model=model, input=batch)\n",
    "        all_embeddings.extend([d.embedding for d in resp.data])\n",
    "    return all_embeddings\n",
    "\n",
    "for file_path in tqdm(file_paths, desc=\"Processing documents\"):\n",
    "    # Step 2: Read content based on file type\n",
    "    if file_path.endswith('.txt'):\n",
    "        text = read_text_file(file_path)\n",
    "    elif file_path.endswith('.pdf'):\n",
    "        text = read_pdf_file(file_path)\n",
    "    else:\n",
    "        print(f\"Unsupported file type: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    # Step 3: Split text into sentences\n",
    "    sentences = split_text_into_sentences(text, data_language)\n",
    "\n",
    "    # Step 4: Chunk sentences into groups\n",
    "    chunks = chunk_sentences(sentences, chunk_size)\n",
    "\n",
    "    # Step 5: Embed each chunk (batch for efficiency)\n",
    "    embeddings = embed_texts(chunks, model=embedding_model_name, batch_size=32)\n",
    "\n",
    "    # Use file name as the document ID and create metadata with chunk index\n",
    "    file_name = os.path.basename(file_path)\n",
    "    ids = []\n",
    "    metadatas = []\n",
    "    for i, _ in enumerate(chunks):\n",
    "        chunk_id = f\"{file_name}_chunk_{i}\"\n",
    "        ids.append(chunk_id)\n",
    "        metadatas.append({\"file_name\": file_name, \"chunk_id\": chunk_id, \"chunk_index\": i})\n",
    "\n",
    "    # Step 6: Add to ChromaDB\n",
    "    collection.add(\n",
    "        ids=ids,\n",
    "        documents=chunks,\n",
    "        embeddings=embeddings,\n",
    "        metadatas=metadatas,\n",
    "    )\n",
    "\n",
    "print(\"Done! Documents embedded and stored in ChromaDB.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d89c8cdf-748f-4f76-b576-537b73e30fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChromaRetriever:\n",
    "    \"\"\"Retrieve documents from a ChromaDB collection using GPUStack embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model: str, db_path: str, db_collection: str, n_results: int) -> None:\n",
    "        self.embedding_model = embedding_model\n",
    "        self.db_path = db_path\n",
    "        self.db_collection = db_collection\n",
    "        self.n_results = n_results\n",
    "\n",
    "        self.client = chromadb.PersistentClient(path=self.db_path)\n",
    "        self.collection = self.client.get_collection(name=self.db_collection)\n",
    "\n",
    "    def _embed(self, text: str) -> list[float]:\n",
    "        resp = client.embeddings.create(model=self.embedding_model, input=[text])\n",
    "        return resp.data[0].embedding\n",
    "\n",
    "    def retrieve(self, query: str):\n",
    "        \"\"\"Embeds the query and retrieves relevant documents from the collection.\"\"\"\n",
    "        try:\n",
    "            embedded_query = self._embed(query)\n",
    "            results = self.collection.query(\n",
    "                query_embeddings=[embedded_query],\n",
    "                n_results=self.n_results\n",
    "            )\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during retrieval: {e}\")\n",
    "            return None\n",
    "\n",
    "    def format_results_for_prompt(self, results) -> str:\n",
    "        \"\"\"Format retrieved chunks into a readable context string for the LLM prompt.\"\"\"\n",
    "        if not results or not results.get(\"documents\"):\n",
    "            return \"No relevant data found.\"\n",
    "\n",
    "        formatted_data = \"\"\n",
    "        for idx, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n",
    "            chunk_id = metadata.get('chunk_id', 'N/A')\n",
    "            file_name = metadata.get('file_name', 'N/A')\n",
    "            formatted_data += f\"Document {idx + 1}:\\n\"\n",
    "            formatted_data += f\"Chunk ID: {chunk_id}\\n\"\n",
    "            formatted_data += f\"File Name: {file_name}\\n\"\n",
    "            formatted_data += f\"Content:\\n{doc}\\n\"\n",
    "            formatted_data += \"-\" * 60 + \"\\n\"\n",
    "        return formatted_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65addc8c-b792-4bff-ae1b-70584a543b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Responder:\n",
    "    \"\"\"Generate responses using a GPUStack-hosted LLM (OpenAI-compatible chat API).\"\"\"\n",
    "\n",
    "    def __init__(self, data: str, model: str, prompt_template: str, query: str) -> None:\n",
    "        self.data = data\n",
    "        self.model = model\n",
    "        self.prompt_template = prompt_template\n",
    "        self.query = query\n",
    "        self.prompt = prompt_template.format(data=self.data, query=self.query)\n",
    "\n",
    "    def generate_response(self) -> str:\n",
    "        \"\"\"One-shot response generation.\"\"\"\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                temperature=0.3,\n",
    "                top_p=1,\n",
    "                max_tokens=800,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": self.prompt}\n",
    "                ],\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"An error occurred during response generation: {e}\")\n",
    "\n",
    "    def stream_response(self):\n",
    "        \"\"\"Stream the response token-by-token (if supported by the server).\"\"\"\n",
    "        try:\n",
    "            stream = client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                temperature=0.3,\n",
    "                top_p=1,\n",
    "                max_tokens=800,\n",
    "                messages=[{\"role\": \"user\", \"content\": self.prompt}],\n",
    "                stream=True,\n",
    "            )\n",
    "            for event in stream:\n",
    "                delta = event.choices[0].delta\n",
    "                if delta and getattr(delta, \"content\", None):\n",
    "                    print(delta.content, end=\"\", flush=True)\n",
    "            print(\"\\n\")\n",
    "        except TypeError:\n",
    "            # Some OpenAI-compatible servers do not support streaming.\n",
    "            print(self.generate_response())\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"An error occurred during streaming: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa7f8039-c85c-4707-9c0b-9f294182d0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type a query to search the DB. Type 'quit' to exit:   what is DSL?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Query Results ---\n",
      "\n",
      "Result 1:\n",
      "Document ID: Support_ Services - Data Science Lab.pdf_chunk_0\n",
      "File Name: Support_ Services - Data Science Lab.pdf\n",
      "Distance: 0.7959156632423401\n",
      "Content:\n",
      "26/11/2024, 13:36 Support: Services - Data Science Lab\n",
      "Data Science Lab\n",
      "Services\n",
      "Whisper\n",
      "DSL provides a container and script to use OpenAI's Whisper speech recognition model on the Ubelix HPC Cluster. Documentation below:\n",
      "Documentation (PDF, 1.1 MB)\n",
      "Grid Certificates\n",
      "DSL operates Swiss National Authority for eScience Grid Certificates as a service. https://www.dsl.unibe.ch/support/services/ 1/1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Result 2:\n",
      "Document ID: Lab_ Publications - Data Science Lab.pdf_chunk_0\n",
      "File Name: Lab_ Publications - Data Science Lab.pdf\n",
      "Distance: 0.9124504327774048\n",
      "Content:\n",
      "26/11/2024, 13:33 Lab: Publications - Data Science Lab\n",
      "Data Science Lab\n",
      "Publications\n",
      "DSL currently does not maintain an own list of publications. For publications of DSL associates, please refer to their personal\n",
      "pages under About or search them in the University publication database BORIS publications. https://www.dsl.unibe.ch/lab/publications/ 1/1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Result 3:\n",
      "Document ID: Support_ Internships - Data Science Lab.pdf_chunk_0\n",
      "File Name: Support_ Internships - Data Science Lab.pdf\n",
      "Distance: 0.9600332975387573\n",
      "Content:\n",
      "26/11/2024, 13:36 Support: Internships - Data Science Lab\n",
      "Data Science Lab\n",
      "Internships\n",
      "The DSL offers internships. If you are a Master or PhD studen, the idea is that you come with your own research objective and\n",
      "data, and the DSL will help you in a structured way. If you are not student, the DSL will help you finding a task. Internships are\n",
      "not paid, but awarded with a DSL internship confirmation letter. If you are interested, please contact support.dsl@unibe.ch or\n",
      "show up directly in person in a Walk-In session. Internships are provided on the basis of a signed agreement between the DSL\n",
      "supervisor and the intern. Internship-Agreement-Template (PDF, 131KB)\n",
      "https://www.dsl.unibe.ch/support/internships/ 1/1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Result 4:\n",
      "Document ID: Support - Data Science Lab.pdf_chunk_0\n",
      "File Name: Support - Data Science Lab.pdf\n",
      "Distance: 0.9629054665565491\n",
      "Content:\n",
      "26/11/2024, 13:36 Support - Data Science Lab\n",
      "Data Science Lab\n",
      "Support\n",
      "Please describe your request shortly in an email to:\n",
      "support.dsl@unibe.ch\n",
      "If you have code, please attach or link that code to the email. You will receive a first response within a working day. Support Examples\n",
      "Support Examples\n",
      "I would like to use machine learning for my task\n",
      "How do I run my analysis on HPC (UBELIX)\n",
      "I need advice on a possible AI solution for my organisation (public or private)\n",
      "I need someone to take care of my servers\n",
      "My analysis code doesn't run\n",
      "I would like to have my analysis code reviewed and optimised\n",
      "I need a programmer for my project\n",
      "Where can I store my research data? I need help with choice of the analysis method\n",
      "I need a web application\n",
      "My group needs help for programming our data acquisition system\n",
      "I need AI and ML expertise/partner for my grant application\n",
      "I need a collaborator for co-analysis\n",
      "... If we cannot help you, we will gladly refer you to someone who can provide the necessary help and support. DSL's support is also open for private and public institutions. Bachelor's and Master's Theses\n",
      "https://www.dsl.unibe.ch/support/support/ 1/2\n",
      "26/11/2024, 13:36 Support - Data Science Lab\n",
      "Bachelor's and Master's Theses\n",
      "We do provide support for Bachelor and Master Students, but if you require help with your thesis, your first point of contact\n",
      "has to be your supervisor. Ideally, they would contact us directly, if applicable, but you can also ask them to refer you to us. If\n",
      "you contact us, please include your supervisor in email communications with us. Rate Policy\n",
      "Rate Policy\n",
      "We are commited to providing access to expertise and cutting-edge solutions in the field of data science, machine learning,\n",
      "artficial intelligence and research IT. For specific inquiries regarding rates or to discuss your project requirements, please don't hesitate to contact us. We are here\n",
      "to support your research endeavors and look forward to collaborating with you. DSL Rate Policy (PDF, 89KB)\n",
      "MIC Support Collaboration\n",
      "MIC Support Collaboration\n",
      "MIC students and affiliates have dedicated image data analysis support. For support requests please use support.dsl@unibe.ch. https://www.dsl.unibe.ch/support/support/ 2/2\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Result 5:\n",
      "Document ID: Support_ Frequently Asked Questions - Data Science Lab.pdf_chunk_0\n",
      "File Name: Support_ Frequently Asked Questions - Data Science Lab.pdf\n",
      "Distance: 1.0936872959136963\n",
      "Content:\n",
      "26/11/2024, 13:36 Support: Frequently Asked Questions - Data Science Lab\n",
      "Data Science Lab\n",
      "Frequently Asked Questions\n",
      "Which licence to use for sharing? Which licence to use for sharing? Which licence to choose when you want to share your software for further usage is regulated by the university here. Basically\n",
      "your group leader has to decide. Without a licence your software cannot legally be modified or redistributed by others. How do I use the HPC cluster (UBELIX) at the university? How do I use the HPC cluster (UBELIX) at the university? The central linux HPC cluster is here. Detailed instructions about how to access and use it can be found here. How can I get SNSF funding for DSL support? How can I get SNSF funding for DSL support? DSL staff can be entered as “Other Employee” in an SNSF application. The DSL has three levels of employees with\n",
      "corresponding salary rates: Early, Advanced and Senior. Level and employment percentage you decide together with DSL,\n",
      "reflecting the tasks in your research plan. If you think it would be good for your project, senior DSL staff may also act as project partners. Senior DSL employees have\n",
      "several years of research and publication background. Publications are not a performance indicator for DSL staff. However, if\n",
      "intellectual contributions justify co-authorship, this is often appreciated. Please contact the DSL director for more information. https://www.dsl.unibe.ch/support/faq/ 1/2\n",
      "26/11/2024, 13:36 Support: Frequently Asked Questions - Data Science Lab\n",
      "Is there a central university service portal for questions?\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type a query to search the DB. Type 'quit' to exit:   quit\n"
     ]
    }
   ],
   "source": [
    "#let's try the retriever by itself first\n",
    "\n",
    "results_numbers = 5\n",
    "\n",
    "retriever = ChromaRetriever(embedding_model=embedding_model_name, \n",
    "                                db_path=db_directory, \n",
    "                                db_collection=collection_name, \n",
    "                                n_results=results_numbers)\n",
    "\n",
    "while True:\n",
    "    query = str(input(\"Type a query to search the DB. Type 'quit' to exit:  \"))\n",
    "\n",
    "    if query.lower() == 'quit':\n",
    "        break\n",
    "    else:\n",
    "        results = retriever.retrieve(query)\n",
    "\n",
    "\n",
    "            # Print out the results\n",
    "        print(\"\\n--- Query Results ---\\n\")\n",
    "        for idx, (doc, metadata, distance) in enumerate(zip(results['documents'][0], results['metadatas'][0], results['distances'][0])):\n",
    "            print(f\"Result {idx + 1}:\")\n",
    "            print(f\"Document ID: {metadata.get('chunk_id', 'N/A')}\")\n",
    "            print(f\"File Name: {metadata.get('file_name', 'N/A')}\")\n",
    "            print(f\"Distance: {distance}\")\n",
    "            print(f\"Content:\\n{doc}\\n\")\n",
    "            print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc5ae6-2871-40db-94d1-43ee77119889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask a question. Type quit to exit:   what kind of support does DSL provide?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking the DB for relevant information .......\n",
      "The Data Science Lab (DSL) offers a **broad, “one‑stop‑shop” of data‑science and research‑IT support**.  According to the information on the DSL website, the Lab can help with:\n",
      "\n",
      "| Area of support | Typical requests |\n",
      "|-----------------|------------------|\n",
      "| **Machine‑learning / AI** | Advice on using ML/AI for a specific task, help designing AI solutions for public or private organisations, AI/ML expertise for grant applications, co‑analysis collaborations. |\n",
      "| **High‑Performance Computing (UBELIX)** | How to access and run analyses on the university’s HPC cluster, including a ready‑to‑use container and script for OpenAI’s **Whisper** speech‑recognition model. |\n",
      "| **Programming & Code** | Debugging code that does not run, code review and optimisation, providing a programmer for a project, building web applications, programming data‑acquisition systems. |\n",
      "| **Data Management** | Advice on where to store research data, choosing appropriate analysis methods, handling image‑data analysis (dedicated support for MIC students and affiliates). |\n",
      "| **Infrastructure & Server Management** | Taking care of servers, general IT support for research projects. |\n",
      "| **Academic Support** | Guidance for Bachelor’s and Master’s theses (through the student’s supervisor), internships for Master/PhD students (you bring your own research objective and data; the Lab provides structured help). |\n",
      "| **Collaboration & Partnerships** | Acting as a project partner, providing co‑authors when intellectual contributions merit it, referring you to external experts if DSL cannot help directly. |\n",
      "| **Administrative Services** | Issuing Swiss National Authority for e‑Science **Grid certificates**, providing information on rate policies and possible funding (e.g., SNSF applications). |\n",
      "\n",
      "If you need any of the above assistance, you should **email support.dsl@unibe.ch** (or info.dsl@unibe.ch for general enquiries) and, when relevant, include your supervisor or other project contacts.  \n",
      "\n",
      "*If the specific support you are looking for is not listed here, the DSL staff will gladly refer you to an appropriate external service.*\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Let's try the whole\n",
    "while True:\n",
    "    retriever = ChromaRetriever(embedding_model=embedding_model_name, \n",
    "                                db_path=db_directory, \n",
    "                                db_collection=collection_name, \n",
    "                                n_results=5)\n",
    "        \n",
    "    user_query = str(input(\"Ask a question. Type quit to exit:  \"))\n",
    "    if user_query.lower() == \"quit\":\n",
    "        break\n",
    "    else:\n",
    "        print(\"Looking the DB for relevant information .......\")\n",
    "        # get the data for the RAG and put it in str format\n",
    "        search_results = retriever.retrieve(user_query)\n",
    "        formated_result = retriever.format_results_for_prompt(search_results)\n",
    "\n",
    "        responder = Responder(data=formated_result, model=llm_model, prompt_template=prompt, query=user_query)\n",
    "        responder.stream_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4479e074-c848-48a3-8821-ea0a335b102d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
