{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c945f271",
   "metadata": {},
   "source": [
    "<b>Simple script to train Hidden Markov Model for Part of Speech tagging using NLTK</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42651d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the models\n",
    "import nltk\n",
    "from nltk import HiddenMarkovModelTagger as hmm # do not use nltk.tag.hmm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import treebank\n",
    "import warnings\n",
    "import dill\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36f2a6f",
   "metadata": {},
   "source": [
    "<b>Download the data. Run only once</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a24fd209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the treebank dataset is downloaded\n",
    "#nltk.download('treebank')\n",
    "#nltk,download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5837486",
   "metadata": {},
   "source": [
    "<b>Prepare the data. We'll use the Penn Treebank which is an English Corpus that includes pos tagging. For information on the tagset: https://www.sketchengine.eu/penn-treebank-tagset/\n",
    "We split the data into training and testing. Try to change the data size and experiment with the accuracy change.</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "734b1d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tagged examples in the dataset is: 3914\n",
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
      "{'RP', 'EX', 'CC', 'VBD', 'NNP', 'JJ', 'RBR', 'VB', 'SYM', 'RBS', '-RRB-', 'MD', 'RB', 'JJR', '-LRB-', '#', 'VBN', 'WP$', 'DT', 'LS', 'VBP', 'POS', 'PDT', 'NN', 'WP', 'PRP', 'VBG', '-NONE-', 'NNS', ',', 'JJS', 'NNPS', 'CD', 'IN', 'VBZ', 'WRB', 'TO', 'PRP$', '$', 'WDT', '.', \"''\", ':', 'UH', '``', 'FW'}\n"
     ]
    }
   ],
   "source": [
    "print(f'The number of tagged examples in the dataset is: {len(treebank.tagged_sents())}')\n",
    "train_data = treebank.tagged_sents()[:2000]\n",
    "test_data = treebank.tagged_sents()[-500:]\n",
    "\n",
    "\n",
    "print(train_data[0])\n",
    "\n",
    "# Extracting unique tags from train_data\n",
    "unique_tags = set(tag for sent in train_data for _, tag in sent)\n",
    "\n",
    "print(unique_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9059b0",
   "metadata": {},
   "source": [
    "<b>Define the trainer and train the model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "727989d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = hmm.train(train_data, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5cc777a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's accuracy on the test data\n",
    "accuracy = tagger.accuracy(test_data)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c09573",
   "metadata": {},
   "source": [
    "<b>Generate true tags list and model prediction to get more detailed stats on where the model performed better and where it didn't perform so well</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe468aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Predictions\n",
    "true_tags = [tag for sent in test_data for _, tag in sent]\n",
    "predicted_tags = [tag for sent in tagger.tag_sents([[word for word, _ in sent] for sent in test_data]) for _, tag in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8133756c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: RP\n",
      "Correct Predictions: 15\n",
      "Wrong Predictions: 9\n",
      "Accuracy: 0.62\n",
      "\n",
      "Label: EX\n",
      "Correct Predictions: 3\n",
      "Wrong Predictions: 0\n",
      "Accuracy: 1.00\n",
      "\n",
      "Label: CC\n",
      "Correct Predictions: 275\n",
      "Wrong Predictions: 1\n",
      "Accuracy: 1.00\n",
      "\n",
      "Label: VBD\n",
      "Correct Predictions: 394\n",
      "Wrong Predictions: 70\n",
      "Accuracy: 0.85\n",
      "\n",
      "Label: NNP\n",
      "Correct Predictions: 927\n",
      "Wrong Predictions: 247\n",
      "Accuracy: 0.79\n",
      "\n",
      "Label: JJ\n",
      "Correct Predictions: 559\n",
      "Wrong Predictions: 148\n",
      "Accuracy: 0.79\n",
      "\n",
      "Label: RBR\n",
      "Correct Predictions: 3\n",
      "Wrong Predictions: 10\n",
      "Accuracy: 0.23\n",
      "\n",
      "Label: VB\n",
      "Correct Predictions: 289\n",
      "Wrong Predictions: 19\n",
      "Accuracy: 0.94\n",
      "\n",
      "Label: RBS\n",
      "Correct Predictions: 1\n",
      "Wrong Predictions: 0\n",
      "Accuracy: 1.00\n",
      "\n",
      "Label: -RRB-\n",
      "Correct Predictions: 14\n",
      "Wrong Predictions: 2\n",
      "Accuracy: 0.88\n",
      "\n",
      "Label: MD\n",
      "Correct Predictions: 132\n",
      "Wrong Predictions: 0\n",
      "Accuracy: 1.00\n",
      "\n",
      "Label: RB\n",
      "Correct Predictions: 235\n",
      "Wrong Predictions: 56\n",
      "Accuracy: 0.81\n",
      "\n",
      "Label: JJR\n",
      "Correct Predictions: 38\n",
      "Wrong Predictions: 6\n",
      "Accuracy: 0.86\n",
      "\n",
      "Label: -LRB-\n",
      "Correct Predictions: 15\n",
      "Wrong Predictions: 1\n",
      "Accuracy: 0.94\n",
      "\n",
      "Label: VBN\n",
      "Correct Predictions: 198\n",
      "Wrong Predictions: 68\n",
      "Accuracy: 0.74\n",
      "\n",
      "Label: WP$\n",
      "Correct Predictions: 2\n",
      "Wrong Predictions: 2\n",
      "Accuracy: 0.50\n",
      "\n",
      "Label: DT\n",
      "Correct Predictions: 1011\n",
      "Wrong Predictions: 10\n",
      "Accuracy: 0.99\n",
      "\n",
      "Label: VBP\n",
      "Correct Predictions: 89\n",
      "Wrong Predictions: 22\n",
      "Accuracy: 0.80\n",
      "\n",
      "Label: POS\n",
      "Correct Predictions: 116\n",
      "Wrong Predictions: 5\n",
      "Accuracy: 0.96\n",
      "\n",
      "Label: PDT\n",
      "Correct Predictions: 2\n",
      "Wrong Predictions: 2\n",
      "Accuracy: 0.50\n",
      "\n",
      "Label: NN\n",
      "Correct Predictions: 1521\n",
      "Wrong Predictions: 307\n",
      "Accuracy: 0.83\n",
      "\n",
      "Label: WP\n",
      "Correct Predictions: 13\n",
      "Wrong Predictions: 0\n",
      "Accuracy: 1.00\n",
      "\n",
      "Label: PRP\n",
      "Correct Predictions: 143\n",
      "Wrong Predictions: 2\n",
      "Accuracy: 0.99\n",
      "\n",
      "Label: VBG\n",
      "Correct Predictions: 120\n",
      "Wrong Predictions: 59\n",
      "Accuracy: 0.67\n",
      "\n",
      "Label: -NONE-\n",
      "Correct Predictions: 834\n",
      "Wrong Predictions: 2\n",
      "Accuracy: 1.00\n",
      "\n",
      "Label: ,\n",
      "Correct Predictions: 572\n",
      "Wrong Predictions: 0\n",
      "Accuracy: 1.00\n",
      "\n",
      "Label: NNS\n",
      "Correct Predictions: 596\n",
      "Wrong Predictions: 113\n",
      "Accuracy: 0.84\n",
      "\n",
      "Label: NNPS\n",
      "Correct Predictions: 2\n",
      "Wrong Predictions: 35\n",
      "Accuracy: 0.05\n",
      "\n",
      "Label: JJS\n",
      "Correct Predictions: 20\n",
      "Wrong Predictions: 3\n",
      "Accuracy: 0.87\n",
      "\n",
      "Label: CD\n",
      "Correct Predictions: 549\n",
      "Wrong Predictions: 82\n",
      "Accuracy: 0.87\n",
      "\n",
      "Label: IN\n",
      "Correct Predictions: 1193\n",
      "Wrong Predictions: 31\n",
      "Accuracy: 0.97\n",
      "\n",
      "Label: VBZ\n",
      "Correct Predictions: 191\n",
      "Wrong Predictions: 25\n",
      "Accuracy: 0.88\n",
      "\n",
      "Label: WRB\n",
      "Correct Predictions: 19\n",
      "Wrong Predictions: 2\n",
      "Accuracy: 0.90\n",
      "\n",
      "Label: TO\n",
      "Correct Predictions: 286\n",
      "Wrong Predictions: 0\n",
      "Accuracy: 1.00\n",
      "\n",
      "Label: PRP$\n",
      "Correct Predictions: 71\n",
      "Wrong Predictions: 1\n",
      "Accuracy: 0.99\n",
      "\n",
      "Label: $\n",
      "Correct Predictions: 154\n",
      "Wrong Predictions: 0\n",
      "Accuracy: 1.00\n",
      "\n",
      "Label: WDT\n",
      "Correct Predictions: 59\n",
      "Wrong Predictions: 0\n",
      "Accuracy: 1.00\n",
      "\n",
      "Label: .\n",
      "Correct Predictions: 491\n",
      "Wrong Predictions: 0\n",
      "Accuracy: 1.00\n",
      "\n",
      "Label: ''\n",
      "Correct Predictions: 51\n",
      "Wrong Predictions: 0\n",
      "Accuracy: 1.00\n",
      "\n",
      "Label: :\n",
      "Correct Predictions: 47\n",
      "Wrong Predictions: 0\n",
      "Accuracy: 1.00\n",
      "\n",
      "Label: ``\n",
      "Correct Predictions: 52\n",
      "Wrong Predictions: 1\n",
      "Accuracy: 0.98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute accuracy for each label\n",
    "labels = list(set(true_tags))\n",
    "for label in labels:\n",
    "    correct_predictions = sum(1 for t, p in zip(true_tags, predicted_tags) if t == label and p == label)\n",
    "    total_predictions = sum(1 for t in true_tags if t == label)\n",
    "    wrong_predictions = total_predictions - correct_predictions\n",
    "    label_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"Correct Predictions: {correct_predictions}\")\n",
    "    print(f\"Wrong Predictions: {wrong_predictions}\")\n",
    "    print(f\"Accuracy: {label_accuracy:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531e62b6",
   "metadata": {},
   "source": [
    "<b>If I'm happy with the model, I can save it for later usage</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "546e6c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "with open('hmm_tagger.pkl', 'wb') as f:\n",
    "    dill.dump(tagger, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7cb897",
   "metadata": {},
   "source": [
    "<b>You can load the model at anytime to use it for tagging sentences</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed608f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model from the file\n",
    "with open('hmm_tagger.pkl', 'rb') as f:\n",
    "    loaded_tagger = dill.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f982b79",
   "metadata": {},
   "source": [
    "<b>Try the model on new text</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b096dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('took', 'VBD'), ('the', 'DT'), ('train', 'NN'), ('from', 'IN'), ('Zurich', '-NONE-'), ('to', 'TO'), ('Italy', 'VB'), ('last', 'JJ'), ('night', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I took the train from Zurich to Italy last night'\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Tag the tokenized sentence\n",
    "tagged_sentence = loaded_tagger.tag(tokens)\n",
    "\n",
    "print(tagged_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b2c485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
