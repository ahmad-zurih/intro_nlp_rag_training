{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24f8cdf9-8b01-4220-b4d6-bc98b2a76901",
   "metadata": {},
   "source": [
    "# Exploring a RAG System with Sentence Transformers, ChromaDB, and Ollama\n",
    "\n",
    "This notebook demonstrates the implementation of a **Retrieval-Augmented Generation (RAG) system** using:\n",
    "- **Sentence Transformers** for embedding text,\n",
    "- **ChromaDB** as the vector database, and\n",
    "- **Ollama** as the LLM for generating responses.\n",
    "\n",
    "The primary goal is to understand how these components interact to enhance information retrieval and generation. \n",
    "\n",
    "> **Note:** This notebook is for **educational purposes** only and is not intended for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f608af0a-93e2-4c69-bd79-620836593927",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install the required packages. Given that ollama api is running on the UBELIX\n",
    "#before running this, open the terminal and run this command in another tab:\n",
    "#singularity exec --nv /storage/research/dsl_shared/solutions/singularity/ollama.sif ollama serve & \n",
    "!pip install pdfplumber==0.11.4\n",
    "!pip install ollama==0.3.3\n",
    "!pip install nltk==3.9.1\n",
    "!pip install sentence-transformers==3.2.1\n",
    "!pip install chromadb==0.5.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe43c0ec-c213-4fcd-b4c7-db88b665dc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import nltk\n",
    "import ollama\n",
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90af85b4-ca96-4630-9e3d-876b6c55cd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc870a33-713a-44fd-a61e-aca431276925",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.pull(\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b735808b-d3ec-40b3-9bc0-56481a7016a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's set some variables\n",
    "\n",
    "model_name = \"Lajavaness/bilingual-embedding-large\"  #choose any embedding model you prefer\n",
    "\n",
    "vector_db = \"chromaDB\" # Allowed Values ['chromaDB', 'FAISS']. Only ChromaDB works now\n",
    "\n",
    "collection_name = \"dsl_embeddings3\"\n",
    "\n",
    "raw_db = \"/storage/homefs/aa22x177/dsl_data\"  #root directory to where raw documents are stored\n",
    "\n",
    "data_language = \"english\" #variable for the tokenizer. Supported language = ['czech', 'danish', 'dutch', 'english', 'estonian', 'finnish', 'french', 'german' ,'greek' ,'italian' ,'norwegian', 'polish' ,'portuguese', 'russian' ,'slovene','spanish', 'swedish', 'turkish']\n",
    "\n",
    "db_directory = os.path.join(os.path.expanduser('~'), '.db')  #default. Change it to where you want to store the vector DB\n",
    "\n",
    "chunk_size = 20 #stands for the number of sentences per chunk\n",
    "\n",
    "llm_model = 'llama3.2:latest' # select any model available on the ollama site https://ollama.com/search\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are a helpful polite assistat that works at the Data Science Lab (DSL). given the following data about DSL: \\n {data} \\n\n",
    "and the following query: \\n{query}\\n\n",
    "generate a responde. If there are no relevant information in the data, state that you don't have an answer and advise to contact DSL through info.dsl@unibe.ch or support.dsl@unibe.ch\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9616d933-5ac0-4276-a072-954fab171cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create helper functions that helps us read into raw data [txt, pdf] in order to convert them into strings and create embeddings\n",
    "\n",
    "def get_file_paths(root_dir: str, file_extensions: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Retrieves a list of paths to all files with specified extensions in the given root directory and its subdirectories.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): The root directory to search for files.\n",
    "        file_extensions (list[str]): A list of file extensions to retrieve. For example, [\"txt\", \"pdf\"]\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of file paths to all matching files found within the root directory and its subdirectories.\n",
    "    \"\"\"\n",
    "    file_paths = []\n",
    "    \n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if any(filename.endswith(f\".{ext}\") for ext in file_extensions):\n",
    "                file_paths.append(os.path.join(dirpath, filename))\n",
    "    \n",
    "    return file_paths\n",
    "\n",
    "\n",
    "\n",
    "def read_text_file(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads the content of a text file and returns it as a single string.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the .txt file to read.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the file as a single string.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    return content\n",
    "\n",
    "\n",
    "def read_pdf_file(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads the content of a PDF file and returns it as a single string.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the PDF file to read.\n",
    "    \n",
    "    Returns:\n",
    "        str: The content of the PDF as a single string.\n",
    "    \"\"\"\n",
    "    text_content = []\n",
    "    \n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            # Extract text from each page\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:  # Ensure the page has text\n",
    "                text_content.append(page_text)\n",
    "    \n",
    "    # Join all pages' text into a single string\n",
    "    return \"\\n\".join(text_content)\n",
    "\n",
    "\n",
    "def split_text_into_sentences(text: str, language: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Splits the given text into a list of sentences using NLTK's sentence tokenizer.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to split into sentences.\n",
    "        language (str): The language of the text for the sentence tokenizer\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of sentences.\n",
    "    \"\"\"\n",
    "    sentences = nltk.sent_tokenize(text, language=language)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def chunk_sentences(sentences: list[str], chunk_size: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Groups a list of sentences into chunks, each containing up to `chunk_size` sentences.\n",
    "\n",
    "    Args:\n",
    "        sentences (list[str]): A list of sentences.\n",
    "        chunk_size (int): The number of sentences per chunk.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of text chunks, each containing up to `chunk_size` sentences.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(sentences), chunk_size):\n",
    "        chunk = \" \".join(sentences[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f57fbc3-7c26-42f0-8c1b-f875c856adff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB client\n",
    "client = chromadb.PersistentClient(path=db_directory)\n",
    "\n",
    "\n",
    "print(\"\\n--- Embedding and Storing Documents in ChromaDB ---\")\n",
    "print(f\"Embedding Model: {model_name}\")\n",
    "print(f\"Chunk Size (sentences per chunk): {chunk_size}\")\n",
    "print(f\"Raw Data Directory: {raw_db}\")\n",
    "print(f\"Vector Database Directory: {db_directory}\\n\")\n",
    "print(f\"Vector Database is: {vector_db}\\n\")\n",
    "\n",
    "# Step 1: Load documents (txt and pdf)\n",
    "file_paths = get_file_paths(raw_db, [\"txt\", \"pdf\"])\n",
    "print(f\"Found {len(file_paths)} files to process.\\n\")\n",
    "\n",
    "# Initialize embedding model\n",
    "embedding_model = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "max_seq_length = embedding_model.max_seq_length  # Typically 512 for older models. Newer ones have larger input size\n",
    "\n",
    "# Create or retrieve the collection in ChromaDB\n",
    "collection = client.get_or_create_collection(collection_name)\n",
    "\n",
    "for file_path in tqdm(file_paths, desc=\"Processing documents\"):\n",
    "    # Step 2: Read content based on file type\n",
    "    if file_path.endswith('.txt'):\n",
    "        text = read_text_file(file_path)\n",
    "    elif file_path.endswith('.pdf'):\n",
    "        text = read_pdf_file(file_path)\n",
    "    else:\n",
    "        print(f\"Unsupported file type: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    # Step 3: Split text into sentences\n",
    "    sentences = split_text_into_sentences(text, data_language)\n",
    "\n",
    "    # Step 4: Chunk sentences into groups\n",
    "    chunks = chunk_sentences(sentences, chunk_size)\n",
    "\n",
    "    # Use file name as the document ID and create metadata with chunk index\n",
    "    file_name = os.path.basename(file_path)\n",
    "    for i, chunk_text in enumerate(chunks):\n",
    "        # Step 5: Embed each chunk\n",
    "        embedding = embedding_model.encode(\n",
    "                chunk_text,\n",
    "                truncation=True,\n",
    "                max_length=max_seq_length\n",
    "        )\n",
    "\n",
    "        # Create a unique ID for each chunk\n",
    "        chunk_id = f\"{file_name}_chunk_{i}\"\n",
    "        collection.add(\n",
    "            documents=[chunk_text],\n",
    "            embeddings=[embedding],\n",
    "            metadatas=[{\"file_name\": file_name, \"chunk_id\": i}],\n",
    "            ids=[chunk_id]\n",
    "            )\n",
    "\n",
    "print(\"\\n--- Embedding and Storage Complete ---\")\n",
    "print(f\"Stored {len(file_paths)} documents in ChromaDB.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c8cdf-748f-4f76-b576-537b73e30fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChromaRetriever:\n",
    "    \"\"\"\n",
    "    A class for retrieving documents from a ChromaDB collection based on semantic similarity using embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_model: str, db_path: str, db_collection: str, n_results: int) -> None:\n",
    "        self.embedding_model = embedding_model\n",
    "        self.db_path = db_path\n",
    "        self.db_collection = db_collection\n",
    "        self.n_results = n_results\n",
    "        self.model = SentenceTransformer(self.embedding_model, trust_remote_code=True)\n",
    "        self.client = chromadb.PersistentClient(path=self.db_path)\n",
    "        self.collection = self.client.get_collection(name=self.db_collection)\n",
    "\n",
    "    def retrieve(self, query: str):\n",
    "        \"\"\"Embeds the query and retrieves relevant documents from the collection.\"\"\"\n",
    "        try:\n",
    "            embedded_query = self.model.encode(query)\n",
    "            results = self.collection.query(\n",
    "                query_embeddings=[embedded_query],\n",
    "                n_results=self.n_results\n",
    "            )\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during retrieval: {e}\")\n",
    "            return None\n",
    "        \n",
    "\n",
    "    def format_results_for_prompt(self, results):\n",
    "        \"\"\"\n",
    "        Formats the retrieval results into a string suitable for the Responder's prompt.\n",
    "\n",
    "        Args:\n",
    "            results: The dictionary returned by the retrieve method.\n",
    "\n",
    "        Returns:\n",
    "            A formatted string containing the retrieved data.\n",
    "        \"\"\"\n",
    "        if not results:\n",
    "            return \"No relevant data found.\"\n",
    "\n",
    "        formatted_data = \"\"\n",
    "        for idx, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n",
    "            chunk_id = metadata.get('chunk_id', 'N/A')\n",
    "            file_name = metadata.get('file_name', 'N/A')\n",
    "            formatted_data += f\"Document {idx + 1}:\\n\"\n",
    "            formatted_data += f\"Document ID: {chunk_id}\\n\"\n",
    "            formatted_data += f\"File Name: {file_name}\\n\"\n",
    "            formatted_data += f\"Content:\\n{doc}\\n\"\n",
    "            formatted_data += \"-\" * 80 + \"\\n\"\n",
    "\n",
    "        return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65addc8c-b792-4bff-ae1b-70584a543b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Responder:\n",
    "    \"\"\"\n",
    "    A class to generate responses using the Ollama LLM within a RAG framework.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: str, model: str, prompt_template: str, query: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Responder instance.\n",
    "\n",
    "        Args:\n",
    "            data: The output from the retriever to be added to the prompt\n",
    "            model: The name of the LLM model to use.\n",
    "            prompt_template: The template string for the prompt.\n",
    "            query: The user's query.\n",
    "        \"\"\"\n",
    "        self.data = data \n",
    "        self.model = model \n",
    "        self.prompt_template = prompt_template\n",
    "        self.query = query\n",
    "\n",
    "        self.prompt = prompt_template.format(data=self.data, query=self.query)\n",
    "\n",
    "    \n",
    "    def generate_response(self) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response based on the query and data.\n",
    "\n",
    "        Returns:\n",
    "            The response generated by the LLM.\n",
    "        \"\"\"\n",
    "        self._check_model()\n",
    "        try:\n",
    "            model_output = ollama.generate(model=self.model, prompt=self.prompt)\n",
    "            return model_output['response']\n",
    "        except KeyError as e:\n",
    "            raise ValueError(f\"Response does not contain expected key: {e}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"An error occurred during response generation: {e}\")\n",
    "        \n",
    "\n",
    "    def stream_response(self):\n",
    "        \"\"\"\n",
    "        Stream a response based on the query and data for a chatbot environment.\n",
    "        \"\"\"\n",
    "        self._check_model()\n",
    "        try:\n",
    "            response_generator = ollama.generate(model=self.model, prompt=self.prompt, stream=True)\n",
    "            \n",
    "            for chunk in response_generator:\n",
    "                print(chunk['response'], end='', flush=True)\n",
    "            print(\"\\n\")\n",
    "            return \"\"\n",
    "\n",
    "        except KeyError as e:\n",
    "            raise ValueError(f\"Response does not contain expected key: {e}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"An error occurred during response generation: {e}\")\n",
    "        \n",
    "    \n",
    "    def stream_response_chunks(self):\n",
    "        \"\"\"\n",
    "        Returns a generator that yields chunks of the response text.\n",
    "        \"\"\"\n",
    "        self._check_model()\n",
    "        try:\n",
    "            response_generator = ollama.generate(model=self.model, prompt=self.prompt, stream=True)\n",
    "            for chunk in response_generator:\n",
    "                yield chunk['response']\n",
    "        except KeyError as e:\n",
    "            raise ValueError(f\"Response does not contain expected key: {e}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"An error occurred during response generation: {e}\")\n",
    "\n",
    "        \n",
    "    def _check_model(self):\n",
    "        \"\"\"\n",
    "        Herper function to check if the specified model is available. If not, attempt to download it.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            models = ollama.list()['models']\n",
    "            model_names = [model['name'] for model in models]\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to retrieve the list of models: {e}\")\n",
    "\n",
    "        if self.model not in model_names:\n",
    "            print(f\"Model '{self.model}' is not downloaded. Attempting to download...\")\n",
    "            try:\n",
    "                ollama.pull(self.model)\n",
    "                print(f\"Successfully downloaded model '{self.model}'.\")\n",
    "            except ollama.ResponseError as e:\n",
    "                raise ValueError(f\"Model '{self.model}' does not exist in the Ollama repository. Please check the model name.\")\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"An error occurred while downloading the model '{self.model}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7f8039-c85c-4707-9c0b-9f294182d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's try the retriever by itself first\n",
    "\n",
    "results_numbers = 5\n",
    "\n",
    "retriever = ChromaRetriever(embedding_model=model_name, \n",
    "                                db_path=db_directory, \n",
    "                                db_collection=collection_name, \n",
    "                                n_results=results_numbers)\n",
    "\n",
    "while True:\n",
    "    query = str(input(\"Type a query to search the DB. Type 'quit' to exit:  \"))\n",
    "\n",
    "    if query.lower() == 'quit':\n",
    "        break\n",
    "    else:\n",
    "        results = retriever.retrieve(query)\n",
    "\n",
    "\n",
    "            # Print out the results\n",
    "        print(\"\\n--- Query Results ---\\n\")\n",
    "        for idx, (doc, metadata, distance) in enumerate(zip(results['documents'][0], results['metadatas'][0], results['distances'][0])):\n",
    "            print(f\"Result {idx + 1}:\")\n",
    "            print(f\"Document ID: {metadata.get('chunk_id', 'N/A')}\")\n",
    "            print(f\"File Name: {metadata.get('file_name', 'N/A')}\")\n",
    "            print(f\"Distance: {distance}\")\n",
    "            print(f\"Content:\\n{doc}\\n\")\n",
    "            print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc5ae6-2871-40db-94d1-43ee77119889",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try the whole\n",
    "while True:\n",
    "    retriever = ChromaRetriever(embedding_model=model_name, \n",
    "                                db_path=db_directory, \n",
    "                                db_collection=collection_name, \n",
    "                                n_results=5)\n",
    "        \n",
    "    user_query = str(input(\"Ask a question. Type quit to exit:  \"))\n",
    "    if user_query.lower() == \"quit\":\n",
    "        break\n",
    "    else:\n",
    "        print(\"Looking the DB for relevant information .......\")\n",
    "        # get the data for the RAG and put it in str format\n",
    "        search_results = retriever.retrieve(user_query)\n",
    "        formated_result = retriever.format_results_for_prompt(search_results)\n",
    "\n",
    "        responder = Responder(data=formated_result, model=llm_model, prompt_template=prompt, query=user_query)\n",
    "        responder.stream_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4479e074-c848-48a3-8821-ea0a335b102d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
